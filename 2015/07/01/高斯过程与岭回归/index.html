<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>高斯过程与岭回归 | $\hat{y}$hat</title>
  <meta name="author" content="Guan-Nan Wang">
  
  <meta name="description" content="complex network, machine learning, Alibaba">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="高斯过程与岭回归"/>
  <meta property="og:site_name" content="$\hat{y}$hat"/>

  
    <meta property="og:image" content="undefined"/>
  

  <link href="/favicon.png" rel="icon">
  <link rel="alternate" href="/atom.xml" title="$\hat{y}$hat" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  

</head>


<body>
  <header id="header" class="inner"><div class="alignleft">
  <h1><a href="/">$\hat{y}$hat</a></h1>
  <h2><a href="/">欢迎留言或者托梦</a></h2>
</div>
<nav id="main-nav" class="alignright">
  <ul>
    
      <li><a href="/">首页</a></li>
    
      <li><a href="/archives">存档</a></li>
    
      <li><a href="/about">关于我</a></li>
    
  </ul>
  <div class="clearfix"></div>
</nav>
<div class="clearfix"></div></header>
  <div id="content" class="inner">
    <div id="main-col" class="alignleft"><div id="wrapper"><article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2015-07-01T14:42:04.000Z"><a href="/2015/07/01/高斯过程与岭回归/">2015-07-01</a></time>
      
      
  
    <h1 class="title">高斯过程与岭回归</h1>
  

    </header>
    <div class="entry">
      
        <blockquote>
<p>高斯过程配上kernal后可以演绎出包罗万象的机器学习技法。本文说说高斯过程到岭回归的推导过程。如果今后有时间，我还会说说岭回归、PCA、Kmeans、SOM之间的关系。</p>
</blockquote>
<a id="more"></a>
<p>高斯过程回归的假设是：
给定一些x的值（特征值），我们对f建模（预测值），并假设对应的这些f值服从 <strong>联合正态分布</strong>。</p>
<p>具体来说，假设我们现在目前拥有一个训练集包含3个样本，$(x_1,f_1),(x_2,f_2),(x_3,f_3)$。这三个$f$可以存在噪声，也可以没有。这里先假设没有，因此$f$的分布可以写为：</p>
<p>$$\begin{bmatrix}
f_1\\
f_2\\
f_3
\end{bmatrix} \sim N\left( \begin{bmatrix}
0\\
0\\
0
\end{bmatrix} , \begin{bmatrix}
K_{11} &amp; K_{12} &amp; K_{13}\\
K_{21} &amp; K_{22} &amp; K_{23}\\
K_{31} &amp; K_{32} &amp; K_{33}
\end{bmatrix}\right)$$</p>
<p>这个公式关键是协方差矩阵$K$。在这里引出了高斯过程的另一个重要假设：
如果两个x比较相似（eg,空间中离得比较近），那么对应的y值的相关性也就较高。具体地说，针对上面的例子，如果$x_1$离$x_2$要比$x_3$更近，那么依据假设 $f_3$和$f_2$ 的correlation 要比 $f_3$和$f_1$的correlation 高。也就是说，协方差矩阵中的每个元素$K_{ij}$为对应的两个$x$值$x_i, x_j$之间的相似性度量。说到相似性度量，本文不准备引出metric learning，仅提及矩阵$K$的一个重要性质：
合法的协方差矩阵就是半正定矩阵 (symmetric) Positive Semi-definite Matrix。
So far so good.
在矩阵$K$中，可以使用kernal来计算相似性（根据Mercer Theorem）。支持向量及SVM中的RBF就是一种常用的kernal。在RBF的定义下，矩阵$K$中元素的计算如下：</p>
<p>$$K_{ij} = e^{-\lambda \|x_i - x_j\|^2}$$</p>
<p>当两个$x$空间中距离无穷远时，那么对应的$y$值的相关性为0，当两个$x$空间中距离为0时，那么对应的$y$值的相关性为1。在3个已知训练样本的基础上，对新来的样本$x^*$的$f^*$求解之前需要具备一个条件，那就是：
<strong>$f^*$ 和 训练集里的$f_1$, $f_2$, $f_3$ 同属于一个（4维的）联合正态分布。用数学语言来表达就是：</strong></p>
<p>$$\begin{bmatrix}
f_1\\
f_2\\
f_3\\
f_*
\end{bmatrix} \sim N\left( \begin{bmatrix}
0\\
0\\
0\\
0
\end{bmatrix} , \begin{bmatrix}
K_{11} &amp; K_{12} &amp; K_{13} &amp; K_{1*}\\
K_{21} &amp; K_{22} &amp; K_{23} &amp; K_{2*}\\
K_{31} &amp; K_{32} &amp; K_{33} &amp; K_{3*}\\
K_{*1} &amp; K_{*2} &amp; K_{*3} &amp; K_{**}
\end{bmatrix}\right)$$</p>
<p>其中，$ \begin{bmatrix}K_{11} &amp; K_{12} &amp; K_{13}\\ K_{21} &amp; K_{22} &amp; K_{23}\\ K_{31} &amp; K_{32} &amp; K_{33}
\end{bmatrix} $ 是原先的矩阵$K$，$ \begin{bmatrix}K_{1*}\\ K_{2*}\\ K_{3*} \end{bmatrix} $ = $ \begin{bmatrix}K_{*1} &amp; K_{*2} &amp; K_{*3} \end{bmatrix} ^T $ 作为矩阵$K_{*}$。直接利用边缘分布性质可以求出$f^*$的后验分布:
$$ f^*(x,f,x_{*}) \sim N \left( \mu*, \sigma* \right) $$</p>
<p>$$\mu^* = K_{*}^{T} K^{-1} \begin{bmatrix}f_1\\ f_2\\ f_3 \end{bmatrix}$$</p>
<p>$$\sigma^* = -K_{*}^{T} K^{-1} K_{*} + K_{**} = -K_{*}^{T} K^{-1} K_{*} + 1$$</p>
<p>至此，无噪声条件下的高斯过程基本就是以上。下面开始推导存在噪声的高斯过程与岭回归之间的关系。存在噪声的预测值的分布：
$$y = f(x) + \epsilon \ \ \ \ \ \ \ \ \epsilon \sim N(0,\sigma^2)$$</p>
<p>$$\begin{bmatrix}
f_1\\
f_2\\
f_3\\
f_*
\end{bmatrix} \sim N\left( \begin{bmatrix}
0\\
0\\
0\\
0
\end{bmatrix} , \begin{bmatrix}
K&#39; &amp; K_{*}\\
K_{*}^T &amp; K_{**}
\end{bmatrix}\right)\ \ \ \ \ K&#39; = K + \sigma^2I$$</p>
<p>因此，</p>
<p>$$ f^*(x,f,x_{*}) \sim N \left( \mu*, \sigma* \right) $$</p>
<p>$$\mu^* = K_{*}^{T} K&#39;^{-1} \begin{bmatrix}f_1\\ f_2\\ f_3 \end{bmatrix}$$</p>
<p>$$\sigma^* = -K_{*}^{T} K&#39;^{-1} K_{*} + K_{**} = -K_{*}^{T} K&#39;^{-1} K_{*} + 1$$</p>
<p>其中，$K_*$是一个3维向量，$K_{**} = 1$，令$K&#39;^{-1} \begin{bmatrix}f_1 &amp; f_2 &amp; f_3 \end{bmatrix}^T$为$\vec \alpha$，那么$\mu^*$就可以改写为：
$$E(f^*) = \mu^* = \sum_{i = 1}^3 {\alpha_i \cdot K(x_i,x_*)}$$，从这个角度看，测试样本预测值的期望是其与现有样本特征Kernal距离的线性组合。与岭回归的目标函数与算式相比：
$$目标函数：\ \ \ \ \hat{\theta} = argmin_{\theta}\|y-x\theta\|_2^2 + \delta^2\|\theta\|^2$$
$$预测函数：\ \ \ \ y_{*} = x_*\theta = x_*X^T[XX^T+\delta I]^-1y$$
把矩阵的点击看做是最简单的核函数即：
$$K(x,x&#39;) = xx^T</p>

      
    </div>
    <footer>
      
        
        
  
  <div class="tags">
    <a href="/tags/小无相功/">小无相功</a>
  </div>

        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>



  <section id="comments">
    <!-- 多说评论框 start -->
    <div class="ds-thread" data-thread-key="2015/07/01/高斯过程与岭回归/" data-title="高斯过程与岭回归" data-url="http://wangguannan.github.io/http://wangguannan.github.io/2015/07/01/高斯过程与岭回归/"></div>
    <!-- 多说评论框 end -->
    <!-- 多说公共JS代码 start (一个网页只需插入一次) -->
    <script type="text/javascript">
    var duoshuoQuery = {short_name:'guannanwang90'};
      (function() {
        var ds = document.createElement('script');
        ds.type = 'text/javascript';ds.async = true;
        ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
        ds.charset = 'UTF-8';
        (document.getElementsByTagName('head')[0] 
         || document.getElementsByTagName('body')[0]).appendChild(ds);
      })();
      </script>
    <!-- 多说公共JS代码 end -->
  </section>
  </div></div>
    <aside id="sidebar" class="alignright">
  <div class="search">
  <form action="//google.com/search" method="get" accept-charset="utf-8">
    <input type="search" name="q" results="0" placeholder="搜索">
    <input type="hidden" name="q" value="site:wangguannan.github.io">
  </form>
</div>

  

  
<div class="widget tag">
  <h3 class="title">标签</h3>
  <ul class="entry">
  
    <li><a href="/tags/Chaos/">Chaos</a><small>1</small></li>
  
    <li><a href="/tags/复杂网络/">复杂网络</a><small>2</small></li>
  
    <li><a href="/tags/小无相功/">小无相功</a><small>1</small></li>
  
    <li><a href="/tags/深度学习/">深度学习</a><small>1</small></li>
  
  </ul>
</div>

</aside>
    <div class="clearfix"></div>
  </div>
  <footer id="footer" class="inner"><div class="alignleft">
  
  &copy; 2015 Guan-Nan Wang
  
</div>
<div class="clearfix"></div></footer>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>


<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>


<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</body>
</html>